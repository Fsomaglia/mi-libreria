# -*- coding: utf-8 -*-
"""mi_libreria

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gkIqE2kqVltF4L2NSOhKcC9NrKJWjKj_
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from scipy import stats
from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score

"""
mi_modulo: resumen
-------------------------

Este módulo proporciona las clases para realizar Analisis Descriptivo, Generar Datos, Regresion Lineal y Regresion Logistica

Clases:
    AnalisisDescriptivo: Realiza estimación de densidad mediante histogramas o núcleos.
    GeneradoraDeDatos: Genera conjuntos de datos con distintas distribuciones y permite obtener su densidad teórica.
    Regresion: Clase base para modelos de regresión.
    RegresionLineal: Implementa una regresión lineal simple.
    RegresionLogistica: Implementa una regresión logística.
"""
class AnalisisDescriptivo:
  """
  Realiza estimación de densidad mediante histogramas o núcleos.
  """
  def __init__(self, datos):
    self.datos = datos

  def calcular_histograma(self, h):
    """
    Calcula un histograma manual con ancho de banda h.
    """
    if self.datos is None:
        raise ValueError("Primero se deben generar los datos con generar_datos().") # Mensaje de aviso si antes no se cargaron los datos
    minimo, maximo = min(self.datos), max(self.datos)                   # Calculamos minimo y maximo
    bordes = np.arange(minimo, maximo + h, h)                           # Calculamos los bordes
    histograma, _ = np.histogram(self.datos, bins=bordes, density=True) # Utilizo el _ porque no me interesa guardar el 2do valor que retorna np.histogram()

    return bordes, histograma

  def evalua_histograma(self, h, x):
      bordes, densidad = self.calcular_histograma(h)
      res = []
      for valor in x:                                   # Para cada valor en x
          for i in range(len(bordes) - 1):              # Recorremos todos los bordes
              if bordes[i] <= valor < bordes[i + 1]:    # Verificamos si el valor esta entre los limites inf y sup de los bordes para cada bin
                  res.append(densidad[i])               # Lo agregamos a la lista que corresponda
                  break                                 # Hacemos un break para terminar ese for que revisa los bins
          else:
              res.append(0)                             # Si está fuera de los intervalos, asignar 0
      return res

  def calcular_densidad_nucleo(self, h, kernel):
      """
      Estima la densidad mediante núcleos.
      """
      x_vals = np.linspace(min(self.datos), max(self.datos), 1000) # Generamos una grilla de 1000 datos.
      n = len(self.datos)
      densidad = np.zeros_like(x_vals, dtype=float)

      # Definir los kernels
      if kernel == 'gaussiano':
          def k_func(u): return (1 / np.sqrt(2 * np.pi)) * np.exp(-0.5 * u**2)
      elif kernel == 'uniforme':
          def k_func(u): return (np.abs(u) <= 0.5).astype(float)
      elif kernel == 'cuadratica':
          def k_func(u): return (3/4 * (1 - u**2) * (np.abs(u) <= 1).astype(float))
      elif kernel == 'triangular':
          def k_func(u): return (1 - np.abs(u)) * (np.abs(u) <= 1).astype(float)
      else:
          raise ValueError("Kernel no reconocido. Usa 'gaussiano', 'uniforme', 'cuadratica' o 'triangular'.")

      for i in range(len(x_vals)):                    # Para cada valor i en el vector
          u = (x_vals[i] - self.datos) / h            # Calculamos el u
          densidad[i] = np.sum(k_func(u)) / (n * h)   # Calculamos la densidad para cada i
      return x_vals, densidad

class GeneradoraDeDatos:
  """
  Genera conjuntos de datos con distintas distribuciones y permite obtener su densidad teórica.
  """
  def __init__(self, n):
      self.n = n
      self.datos = None

  def generar_datos(self, distribucion="normal", loc=0, scale=1):
      """
      Genera datos de la distribución seleccionada.
      """
      if distribucion == "normal":
          self.datos = np.random.normal(loc=loc, scale=scale, size=self.n) # Aplica la generacion para distribucion normal
      elif distribucion == "uniforme":
          self.datos = np.random.uniform(low=loc, high=scale, size=self.n) # Aplica la generacion para distribucion uniforme
      elif distribucion == "bs":
          self.datos = self.generar_datos_bs() # Aplica la generacion para distribucion bs, llamando a la funcion generar_datos_bs()
      else:
          raise ValueError("Distribución no reconocida. Usa 'normal', 'uniforme' o 'bs'.") # Utilizo el raise para devolver un mensaje de error si la palabra es incorrecta.

  def generar_datos_bs(self):
      """
      Genera datos con la distribución BS. Funciona internamente en la funcion generar_datos() cuando se elige la distribución bs.
      """
      u = np.random.uniform(size=(self.n,))
      y = u.copy()
      ind = np.where(u > 0.5)[0]
      y[ind] = np.random.normal(0, 1, size=len(ind))
      for j in range(5):
          ind = np.where((u > j * 0.1) & (u <= (j+1) * 0.1))[0]
          y[ind] = np.random.normal(j/2 - 1, 1/10, size=len(ind))
      return y

  def obtener_densidad_teorica(self, x, tipo="normal"):
      """
      Obtiene la densidad teórica de la distribución generada. Con "normal" como valor default.
      """
      if self.datos is None:
          raise ValueError("Primero se deben generar los datos.")

      if tipo == "normal":
          mu, sigma = np.mean(self.datos), np.std(self.datos)
          return stats.norm.pdf(x, loc=mu, scale=sigma)
      elif tipo == "bs":
          return 0.5 * stats.norm.pdf(x, 0, 1) + 0.1 * sum([stats.norm.pdf(x, j/2 - 1, 1/10) for j in range(5)])
      elif tipo == "uniforme":
          a, b = min(self.datos), max(self.datos)
          return np.ones_like(x) / (b - a) * ((x >= a) & (x <= b))
      else:
          raise ValueError("Tipo debe ser 'normal', 'bs' o 'uniforme'.")

class Regresion:
  """
  Clase base para modelos de regresión.
  """
  def __init__(self, X, y):
    """
    Inicializa los atributos de la clase.
    """
    self.X = X
    self.y = y
    self.X_train = None
    self.y_train = None
    self.X_test = None
    self.y_test = None
    self.modelo = None
    self.resultados = None
    self.betass = None
    self.errores_estandar = None
    self.t_obs = None
    self.p_valor = None

  def predecir(self, X_nuevo):
    """
    Predecir valores de respuesta ante nuevas entradas.

    Args:
        X_nuevo: Nuevos valores de las variables predictoras.
    """
    return self.resultados.predict(X_nuevo)

  def mostrar_parametros(self):
    """
    Mostrar los parámetros del modelo.
    """
    dic_datos = {
        "Beta": self.betass,
        "Std Error": self.errores_estandar,
        "t_obs": self.t_obs,
        "p_valor": self.p_valor
    }
    dataframe = pd.DataFrame(dic_datos)
    return dataframe

  def mostrar_estadisticas(self):
    """
    Mostrar las estadísticas del modelo.
    """
    return self.resultados.summary()

  def _extraer_estadisticas(self):
    """
    Almacena y retorna los valores de los betas, errores standars, t_obs y p-valor (En ese orden).
    """
    self.betass = self.resultados.params
    self.errores_estandar = self.resultados.bse
    self.t_obs = self.resultados.tvalues
    self.p_valor = self.resultados.pvalues
    return self.betass, self.errores_estandar, self.t_obs, self.p_valor

  def asignar_test(self, X_test, y_test):
    self.X_test = X_test
    self.y_test = y_test

  def asignar_train (self, X_train, y_train):
    self.X_train = X_train
    self.y_train = y_train

class RegresionLineal(Regresion):
  """
  Clase hija de Regresion. Sirve para realizar cálculos de regresión lineal simple utilizando la librería statsmodels.
  """
  def ajustar_modelo_lineal(self):
    """
    Iniciliza los valores de Regresion Lineal en base a lo datos de Regresion.
    """
    self.modelo = sm.OLS(self.y, self.X)  # Preparo el modelo
    self.resultados = self.modelo.fit()   # Entreno el modelo
    self._extraer_estadisticas()          # Exraer estadisticas??

  def graficar_dispersion_y_recta(self):
    """
    Grafica la dispersión de puntos (en base a los datos) y la recta de mejor ajuste. Cuando ambas son cuanti.
    """
    for col in self.X.columns[1:]:
        plt.figure()
        sns.scatterplot(x=self.X[col], y=self.y)  # Utilizo el [col] para acceder a todo los valores en caso de que sean muchas variables.
        pred = self.resultados.predict(self.X)    # Predice
        sns.lineplot(x=self.X[col], y=pred)       # Grafica la recta
        plt.title(f'{col} vs y')
        plt.show()

  def graficar_anova(self, variable_categorica_original):
    """
    Grafica de forma adecuada para un modelo con variables categóricas. Util para ANOVA.
    Requiere pasar la variable categórica original (como una Serie).
    """
    df = pd.DataFrame({'ventas': self.y, 'categoria': variable_categorica_original})
    plt.figure(figsize=(8,5))
    sns.boxplot(x='categoria', y='ventas', data=df)
    sns.stripplot(x='categoria', y='ventas', data=df, color='black', alpha=0.4, jitter=0.15)
    plt.title("Distribución de ventas por altura del producto")
    plt.show()

  def coef_correlacion(self):
    """
    Calcula el coeficiente de correlación entre la variable predictora y la variable respuesta
    """
    for col in self.X.columns[1:]:
        corr = np.corrcoef(self.X[col], self.y)[0, 1]
        print(f"Coeficiente de correlación ({col}): {corr:.4f}")

  def residuos_analisis(self):
    """
    Realiza el análisis de los residuos.
    Graficar qqPlot y residuos vs valores predichos.
    """
    residuos = self.resultados.resid
    predichos = self.resultados.predict(self.X)

    # QQ Plot mejorado
    sm.qqplot(residuos, line='45', dist=stats.norm, scale=residuos.std(), loc=residuos.mean()) # Ajusto la distancia y las escalas para que se vean bien en el gráfico
    plt.title("QQ Plot de residuos")
    plt.xlim(residuos.min()*1.5, residuos.max()*1.5)  # Acomodo el zoom en eje X
    plt.ylim(residuos.min()*1.5, residuos.max()*1.5)  # Acomodo el zoom en eje Y
    plt.grid(True)
    plt.show()

    # Residuos vs predicciones
    plt.figure()
    plt.scatter(predichos, residuos)
    plt.axhline(0, color='red', linestyle='--')
    plt.title("Residuos vs Valores predichos")
    plt.xlabel("Valores predichos")
    plt.ylabel("Residuos")
    plt.grid(True)
    plt.show()

  def intervalos_confianza(self):
    """
    Calcula intervalos de confianza.
    Retorna los intervalos de confianza en tabla. | Utilizarlo con print(modelo.intervalos_confianza())
    """
    # Intervalos de confianza para los betas
    intervalo_confianza = self.resultados.conf_int()
    intervalo_confianza.columns = ['Conf. Inf.', 'Conf. Sup.']

    return intervalo_confianza
  def intervalos_prediccion(self, X_nuevo):
    """
    Calcula intervalos de predicción.
    Retorna los intervalos de prediccion.
    Args:
        X_nuevo: Nuevos valores de las variables predictoras.
    """
    # Intervalos de predicción para nuevas observaciones
    X_nuevo = sm.add_constant(X_nuevo)
    predicciones = self.resultados.get_prediction(X_nuevo)
    prediccion_resumen = predicciones.summary_frame(alpha=0.05)  # incluye predicción puntual + IC + IP

    return prediccion_resumen

  def R2_y_R2ajustado(self):
    """
    Calcula el coeficiente de determinación (R cuadrado) y el R cuadrado ajustado.
    Retorna un diccionario con los valores de R cuadrado y R cuadrado ajustado. (En ese orden)
    """
    dic_r2_r2adj = {
        "R2": self.resultados.rsquared,
        "R2_ajustado": self.resultados.rsquared_adj
    }
    df = pd.DataFrame(dic_r2_r2adj)
    return df

class RegresionLogistica(Regresion):
  def ajustar_modelo(self):
    """
    Ajusta el modelo de regresión logística usando statsmodels.
    """
    self.modelo = sm.Logit(self.y, self.X)
    self.resultados = self.modelo.fit()
    self._extraer_estadisticas()

  def evaluar_clasificador(self, umbral=0.5):
    """
    Calcular matriz de confusion, error total de mala clasificacion, sensibilidad y especificidad
    Retorna un diccionario con los valores de Matriz de confusion, sensibilidad, especificidad y error total. (En ese orden)

    Args:
        umbral: Umbral para clasificar los datos.
    """
    y_prob = self.resultados.predict(self.X_test)
    y_pred = (y_prob >= umbral).astype(int)
    matriz = pd.crosstab(y_pred, self.y_test, rownames=['Predicción'], colnames=['Real'])
    # Accedemos de forma segura
    tn = matriz.loc[0, 0]
    fp = matriz.loc[1, 0]
    fn = matriz.loc[0, 1]
    tp = matriz.loc[1, 1]

    sensibilidad = tp / (tp + fn)
    especificidad = tn / (tn + fp)
    error_total = (fp + fn) / len(self.y_test)

    return {
        "Matriz de Confusión": matriz,
        "Sensibilidad": sensibilidad,
        "Especificidad": especificidad,
        "Error Total": error_total
    }

  def predecir(self, X_nuevo, umbral=0.5):
    """
    Predecir valores de respuesta ante nuevas entradas.

    Args:
        X_nuevo: Nuevos valores de las variables predictoras.
        umbral: Umbral para clasificar los datos.
    """
    X_nuevo = sm.add_constant(X_nuevo)
    probs = self.resultados.predict(X_nuevo)
    return (probs >= umbral).astype(int)

  def curva_roc(self):
    """
    Calcula y grafica la curva ROC, el AUC, y determina el punto de corte óptimo (índice de Youden).
    Retorna un diccionario con AUC, evaluación cualitativa, punto de corte óptimo, sensibilidad, especificidad e índice de Youden máximo.
    """
    y_prob = self.resultados.predict(self.X_test)
    fpr, tpr, p_values = roc_curve(self.y_test, y_prob)
    auc = roc_auc_score(self.y_test, y_prob)

    # Calcular índice de Youden
    indice_youden = tpr - fpr
    indice_optimo = np.argmax(indice_youden)
    p_optimo = p_values[indice_optimo]
    sensibilidad_optima = tpr[indice_optimo]
    especificidad_optima = 1 - fpr[indice_optimo]
    youden_max = indice_youden[indice_optimo]

    # Graficar curva ROC
    plt.figure()
    plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.scatter(fpr[indice_optimo], tpr[indice_optimo], color='red', label='Punto óptimo', zorder=10)
    plt.xlabel("Falso Positivo (1 - Especificidad)")
    plt.ylabel("Verdadero Positivo (Sensibilidad)")
    plt.title("Curva ROC")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Evaluación del AUC
    if auc >= 0.9:
        evaluacion = "Excelente (0.9 a 1)"
    elif auc >= 0.8:
        evaluacion = "Muy bueno (0.8 a 0.9)"
    elif auc >= 0.7:
        evaluacion = "Bueno (0.7 a 0.8)"
    elif auc >= 0.6:
        evaluacion = "Regular (0.6 a 0.7)"
    else:
        evaluacion = "Pobre (0.5 a 0.6)"

    return {
        "AUC": auc,
        "Evaluación": evaluacion,
        "p óptimo": p_optimo,
        "Sensibilidad en óptimo": sensibilidad_optima,
        "Especificidad en óptimo": especificidad_optima,
        "Índice de Youden": youden_max
    }


def evaluar_anova(modelo_m, modelo_M):
  '''
  Funcion externa para evaluar anova.
  Retorna: tabla con valores. Utilizar print() en su llamado.
  Ejemplo: print(evaluar_anova(modelo_m, modelo_M))
  Args:
    modelo_m: modelo nulo | X_nulo = sm.add_constant([1] * len(datos))
    modelo_M: modelo con todas/algunas variables. Recordar Dummies | x = pd.get_dummies(datos['...'], drop_first=True).astype(int) | X = sm.add_constant(x)

  '''
  anova_resultado = anova_lm(modelo_m.resultados, modelo_M.resultados)
  return anova_resultado

def importar_librerias():
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import seaborn as sns
  import statsmodels.api as sm
  from statsmodels.stats.anova import anova_lm
  from scipy import stats
  from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score

def separar_datos(datos, porcentaje):
  '''
  Funcion para separar datos en train y test.
  Retorna: datos_train, datos_test. Dos dataframes con los datos separados.
  Args:
    datos: dataframe con los datos que se van a separar
    porcentaje: porcentaje de separacion
  '''
  ## Completar
  import random
  n = datos.shape[0]
  n_train=int(n*porcentaje)
  n_test=n-n_train
  # random.seed(10)
  cuales = random.sample(range(n), n_train) # Elegimos numeros aleatorios

  datos_test = datos.drop(cuales)
  datos_train = datos.iloc[cuales]
  return datos_train, datos_test